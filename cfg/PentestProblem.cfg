# General-purpose settings.
verbose = true
logPath = /home/abhaas/Desktop/testing
overwriteExistingLogFiles = true
logFilePostfix = pentest_problem_test
saveParticles = true


############################## PLUGIN SHARED LIBRARIES TO LOAD #############################

[plugins]

executionInitialBeliefPlugin = libPentestProblemInitialBeliefPlugin.so
planningInitialBeliefPlugin = libPentestProblemInitialBeliefPlugin.so

planningObservationPlugin = libPentestProblemObservationPlugin.so
executionObservationPlugin = libPentestProblemObservationPlugin.so

planningRewardPlugin = libPentestProblemRewardPlugin.so
executionRewardPlugin = libPentestProblemRewardPlugin.so

planningTransitionPlugin = libPentestProblemTransitionPlugin.so
executionTransitionPlugin = libPentestProblemTransitionPlugin.so

heuristicPlugin = libPentestProblemHeuristicPlugin.so

planningTerminalPlugin = libPentestProblemTerminalPlugin.so
executionTerminalPlugin = libPentestProblemTerminalPlugin.so





############################ PLUGIN SPECIFIC OPTIONS ##################################
[generalOptions]
nMachineParams = 4

[initialBeliefOptions]
nMachines = 3
randomizeInitStates = false
inputMachineStates = [[2.0, 1.0, 1.0, 0.0], [0.0, 1.0, 1.0, 0.0], [1.0, 1.0, 1.0, 0.0]]

[transitionPluginOptions]
pSuccessfulHack = 0.9
pSuccessOpenPortNoise = 0.9

[observationPluginOptions]
#Observaion Error (in this case there is none but could add support for it)
observationNoise = 0.05

[rewardPluginOptions]
hackReward = 3.0

failedHackPenalty = -0.8
alreadyHackPenalty = -0.6
stepPenalty = -0.1

################################### EXPERIMENT SPECIFIC OPTIONS #############################
[problem]
# Number of simulation runs
nRuns = 1

# Maximum number of steps to reach the goal
nSteps = 50


############# OPTIONAL GAZEBO Environment Specifications for physics engine and viewer #################################

# The planning environment SDF
planningEnvironmentPath = PentestEnvironment.sdf

# The execution environment SDF
executionEnvironmentPath = PentestEnvironment.sdf

# The robot SDF model
robotName = Player

# Logs the gazebo states
enableGazeboStateLogging = false

# The discount factor of the reward model
discountFactor = 0.98

# Using state- action- and observation spaces that are normalized to [0, 1]
normalizedSpaces = false

allowCollisions = false

# The maximum time to spend on each step, in milliseconds (0 => no time limit)
stepTimeout = 2000


[state]
# State space description
# PENTEST PROBLEM CAN BE DEFINED AS A 1-D dimension with 2 states
additionalDimensions = 2
#additionalDimensionLimits = [[0, 1]]


[action]
# Action space is only x,y displacement for the pedestrian
additionalDimensions = 1
additionalDimensionLimits = [[0, 2]]



[observation]
# The observation space is the same space as the state space
additionalDimensions = 1
additionalDimensionLimits = [[0, 1]]



[changes]
hasChanges = false
changesPath =
areDynamic = false

[ABT]
# The number of trajectories to simulate per time step (0 => wait for timeout)
historiesPerStep = 10000

# If this is set to "true", ABT will prune the tree after every step.
pruneEveryStep = true

# If this is set to "true", ABT will reset the tree instead of modifying it when
# changes occur.
resetOnChanges = false

# The particle filter to use
particleFilter = observationModel

# The maximum depth to search in the tree, relative to the current belief.
maximumDepth = 8

# The minimum number of particles for the current belief state in a simulation.
# Extra particles will be resampled via a particle filter if the particle count
# for the *current* belief state drops below this number during simulation.
minParticleCount = 10000

# True if the above horizon is relative to the initial belief, and false
# if it's relative to the current belief.
isAbsoluteHorizon = false

searchStrategy = ucb(2.0)

estimator = max()

heuristicTimeout = 0.1

actionType = discrete
numInputStepsActions = 18

observationType = discrete
numInputStepsObservation = 6

# The maximum L2-distance between observations for them to be considered similar
# Implementation is set to either 10000 or 0. So any value in between is fine
maxObservationDistance = 100

resetPolicy = false
